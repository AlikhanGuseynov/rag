import os
import gradio as gr
import openai
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    StorageContext,
    load_index_from_storage,
)
from llama_index.core.node_parser import SentenceSplitter
from openai import OpenAI

# üîê –£–∫–∞–∂–∏ —Å–≤–æ–π –∫–ª—é—á
os.environ["OPENAI_API_KEY"] = ""  # –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ–π

openai.api_key = os.environ["OPENAI_API_KEY"]

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏
DOCS_DIR = "docs"
PERSIST_DIR = "storage"
MODEL_NAME = "gpt-3.5-turbo"

# === –°–æ–∑–¥–∞–Ω–∏–µ/–∑–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞

def load_or_build_index():
    if not os.path.exists(PERSIST_DIR):
        print("üîç –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
        documents = SimpleDirectoryReader(DOCS_DIR).load_data()
        node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=100)

        index = VectorStoreIndex.from_documents(
            documents,
            transformations=[node_parser],
        )
        index.storage_context.persist(persist_dir=PERSIST_DIR)
    else:
        print("üìÜ –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞...")
        storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
        index = load_index_from_storage(storage_context)
    return index

# === –§—É–Ω–∫—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã
index = load_or_build_index()
retriever = index.as_retriever(similarity_top_k=3)


def chat_fn(message, history):
    # –ò—â–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
    nodes = retriever.retrieve(message)
    max_chars_per_chunk = 800
    context = "\n\n".join([node.node.get_content()[:max_chars_per_chunk] for node in nodes])

    # –ó–∞–ø—Ä–æ—Å –∫ OpenAI –Ω–∞–ø—Ä—è–º—É—é
    client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç..."},
            {"role": "user", "content": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–í–æ–ø—Ä–æ—Å:\n{message}"}
        ],
        temperature=0,
    )

    answer = response.choices[0].message.content.strip()
    usage = response.usage
    token_line = (
        f"\n\nüî¢ –¢–æ–∫–µ–Ω—ã ‚Äî prompt: {usage.prompt_tokens}, "
        f"–æ—Ç–≤–µ—Ç: {usage.completion_tokens}, –≤—Å–µ–≥–æ: {usage.total_tokens}"
    )
    return answer + token_line

# === –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å
iface = gr.ChatInterface(
    fn=chat_fn,
    title="üìÑ –í–æ–ø—Ä–æ—Å—ã –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É",
    description="–ó–∞–¥–∞–π –≤–æ–ø—Ä–æ—Å –ø–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É (docs/)",
    theme="default",
    chatbot=gr.Chatbot(height=400),
    textbox=gr.Textbox(placeholder="–ù–∞–ø–∏—à–∏ –≤–æ–ø—Ä–æ—Å...", container=False, scale=7),
)

iface.launch()
